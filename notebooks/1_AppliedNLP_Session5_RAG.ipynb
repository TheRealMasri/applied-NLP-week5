{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4daedf",
   "metadata": {},
   "source": [
    "# Session 5 â€” Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "In this notebook, we build a **minimal RAG pipeline** using Lewis Carroll's two Alice books as our corpus as usual:\n",
    "\n",
    "- *Alice's Adventures in Wonderland*\n",
    "- *Through the Looking-Glass*\n",
    "\n",
    "> **IMPORTANT**: It is **highly recommended** to use a virtual environment for this session!  \n",
    "> The packages and downloaded models (embeddings, transformers) can easily reach over **1 GB** in size.  \n",
    "> Using a venv keeps your system clean and makes it easy to manage these large dependencies and delete them when not needed anymore.\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances LLM responses by:\n",
    "1. **Retrieving** relevant information from a knowledge base (your documents)\n",
    "2. **Augmenting** the LLM prompt with this retrieved context\n",
    "3. **Generating** an answer based on both the question and the retrieved information\n",
    "\n",
    "This approach allows LLMs to answer questions about documents they weren't trained on, and reduces hallucinations by grounding responses in actual source material.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "We will:\n",
    "\n",
    "1. **Load** the two books as plain text  \n",
    "2. **Split** them into **overlapping chunks** (text segmentation)  \n",
    "3. **Create embeddings** for each chunk (convert text to vectors)  \n",
    "4. **Store** them in a **vector database (FAISS)** for efficient similarity search  \n",
    "5. **Build** a **retrieval + generation chain** to answer questions about the books  \n",
    "6. **Query** the system with natural language questions\n",
    "\n",
    "The focus is on understanding the *pipeline*, not on perfect model choices. You can swap components (embeddings, LLMs, vector stores) as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83a5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# LangChain components\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Hub for pulling prompts\n",
    "from langsmith import Client\n",
    "hub = Client()\n",
    "\n",
    "# LLM: we use Ollama (local) here to avoid API keys\n",
    "# Make sure you have installed and started Ollama, and pulled a model, e.g.:\n",
    "#   - install from https://ollama.com\n",
    "#   - in a terminal, run: `ollama pull llama3.2`\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# A small helper for nicer printing\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1220a1c",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "### LLM Options\n",
    "\n",
    "In this notebook, we use **Ollama** for local LLM inference (no API keys required).\n",
    "\n",
    "**Alternative LLM options:**\n",
    "- **OpenAI**: `from langchain_openai import ChatOpenAI` â†’ requires API key\n",
    "- **Groq**: `from langchain_groq import ChatGroq` â†’ requires API key  \n",
    "- **Anthropic**: `from langchain_anthropic import ChatAnthropic` â†’ requires API key\n",
    "- **HuggingFace**: `from langchain_huggingface import HuggingFaceEndpoint` â†’ requires API key\n",
    "\n",
    "**To use Ollama:**\n",
    "1. Install from [https://ollama.com/download](https://ollama.com/download)\n",
    "2. Run in terminal: `ollama pull llama3.2` (or another model)\n",
    "3. Ollama runs on `localhost:11434` by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b93f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/nargeschinichian/Desktop/Teaching_Uni/SRH/Applied_NLP/applied-NLP-week5/data\n",
      "Using LLM model: llama3.2 (Ollama)\n"
     ]
    }
   ],
   "source": [
    "# Paths to the Alice books (plain text)\n",
    "# Adjust these paths if your files live somewhere else.\n",
    "DATA_DIR = Path(\"../data\")\n",
    "WONDERLAND_PATH = DATA_DIR / \"Wonderland.txt\"\n",
    "LOOKING_GLASS_PATH = DATA_DIR / \"Looking-Glass.txt\"\n",
    "\n",
    "# Set up local LLM via Ollama\n",
    "# If you prefer Groq or OpenAI, you can swap this block for your own client.\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.0  # Controls randomness: 0.0 = deterministic, 1.0 = creative\n",
    ")\n",
    "\n",
    "# print(\"Data directory:\", DATA_DIR.resolve())\n",
    "# print(\"Using LLM model:\", \"llama3.2 (Ollama)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00ff56",
   "metadata": {},
   "source": [
    "### Configuration Notes\n",
    "\n",
    "**Model Selection:**\n",
    "- `llama3.2`: Fast, good for local testing (3B parameters)\n",
    "- Other Ollama models: `llama3.1`, `mistral`, `phi3` (run `ollama list` to see installed models)\n",
    "\n",
    "**Temperature Setting:**\n",
    "- `temperature=0.0`: Deterministic responses (same answer every time)\n",
    "- `temperature=0.7`: More creative/varied responses\n",
    "- `temperature=1.0`: Maximum creativity (may be less factual)\n",
    "\n",
    "For RAG applications, **lower temperatures (0.0-0.3)** are recommended to keep answers focused on retrieved content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838ebd9",
   "metadata": {},
   "source": [
    "## 1. Load books\n",
    "\n",
    "We reuse the idea of the **`load_book`** helper from earlier sessions, but keep it simple:\n",
    "\n",
    "**Steps:**\n",
    "1. **Read** the text file from disk\n",
    "2. **Strip** Project Gutenberg header/footer (boilerplate text)\n",
    "3. **Return** clean text ready for processing\n",
    "\n",
    "**Why clean the text?**\n",
    "- Project Gutenberg files contain legal notices and metadata\n",
    "- These sections aren't part of the actual book content\n",
    "- Including them would pollute our embeddings with irrelevant information\n",
    "\n",
    "**Data Sources:**\n",
    "- You can use any plain text files (`.txt`)\n",
    "- For other formats: PDF â†’ use `PyPDF2` or `pdfplumber`, DOCX â†’ use `python-docx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498325ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice's Adventures in Wonderland: 144,481 characters after cleaning\n",
      "Through the Looking-Glass: 161,373 characters after cleaning\n"
     ]
    }
   ],
   "source": [
    "def load_book(filepath: Path, name: str) -> str:\n",
    "    # Load and roughly clean a Project Gutenberg text file.\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Very simple cleaning: try to cut away Gutenberg boilerplate\n",
    "    start_markers = [\"CHAPTER I\", \"*** START OF\"]\n",
    "    end_markers = [\"*** END OF\", \"End of Project Gutenberg\"]\n",
    "\n",
    "    start_idx = 0\n",
    "    for marker in start_markers:\n",
    "        if marker in text:\n",
    "            start_idx = text.find(marker)\n",
    "            break\n",
    "\n",
    "    end_idx = len(text)\n",
    "    for marker in end_markers:\n",
    "        if marker in text:\n",
    "            end_idx = text.find(marker)\n",
    "            break\n",
    "\n",
    "    cleaned = text[start_idx:end_idx].strip()\n",
    "    print(f\"{name}: {len(cleaned):,} characters after cleaning\")\n",
    "    return cleaned\n",
    "\n",
    "wonderland_text = load_book(WONDERLAND_PATH, \"Alice's Adventures in Wonderland\")\n",
    "looking_glass_text = load_book(LOOKING_GLASS_PATH, \"Through the Looking-Glass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cc3da0",
   "metadata": {},
   "source": [
    "## 2. Chunk the texts for retrieval\n",
    "\n",
    "Large documents are **too long** to embed and retrieve as a single vector.  \n",
    "Instead, we split the books into **overlapping chunks**:\n",
    "\n",
    "### Parameters Explained\n",
    "\n",
    "- **`chunk_size`**: Maximum number of characters per chunk (default: 800)\n",
    "  - This is a **hard limit** - chunks won't exceed this size\n",
    "  - Too small â†’ loses context, more chunks to search\n",
    "  - Too large â†’ less precise retrieval, may exceed embedding model limits\n",
    "  - **Typical range**: 500-1500 characters\n",
    "\n",
    "- **`chunk_overlap`**: How much neighboring chunks overlap (default: 150)\n",
    "  - Ensures sentences near boundaries aren't split awkwardly\n",
    "  - Helps maintain context across chunk boundaries\n",
    "  - **Typical range**: 10-20% of chunk_size\n",
    "\n",
    "- **`separators`**: Priority order for splitting points\n",
    "  - These determine **where** to split when approaching the chunk_size limit\n",
    "  - The splitter tries each separator in order to find a natural break point:\n",
    "    1. `\\n\\n` â†’ paragraph breaks (preferred - most context preserved)\n",
    "    2. `\\n` â†’ line breaks\n",
    "    3. `. ` â†’ sentence endings\n",
    "    4. ` ` â†’ word boundaries (last resort)\n",
    "  - **Key point**: The splitter builds chunks up to ~800 chars, then looks for the best separator to split on\n",
    "\n",
    "### How It Works Together\n",
    "\n",
    "Example: If text reaches 780 characters, the splitter looks for the first `\\n\\n` (paragraph break). If found, it splits there (even if only 750 chars). If not found, it tries `\\n`, then `. `, then ` `. This keeps chunks **under 800 chars** while breaking at **natural boundaries**.\n",
    "\n",
    "### Experimentation\n",
    "\n",
    "Try adjusting these values to see how they affect:\n",
    "- Number of chunks created\n",
    "- Retrieval quality\n",
    "- Answer accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86b7254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wonderland: 240 chunks (chunk_size=800, overlap=150)\n",
      "Looking-Glass: 255 chunks (chunk_size=800, overlap=150)\n",
      "Total chunks in corpus: 495\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text: str, book_name: str, chunk_size: int = 800, chunk_overlap: int = 150):\n",
    "    # Split a long text into overlapping chunks using RecursiveCharacterTextSplitter.\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],  # try to keep chunks on sentence/paragraph boundaries\n",
    "    )\n",
    "    docs = splitter.create_documents([text])\n",
    "    print(f\"{book_name}: {len(docs)} chunks (chunk_size={chunk_size}, overlap={chunk_overlap})\")\n",
    "    return docs\n",
    "\n",
    "wonderland_chunks = chunk_text(wonderland_text, \"Wonderland\")\n",
    "looking_glass_chunks = chunk_text(looking_glass_text, \"Looking-Glass\")\n",
    "\n",
    "# Combine chunks from both books into a single corpus\n",
    "all_chunks = wonderland_chunks + looking_glass_chunks\n",
    "print(\"Total chunks in corpus:\", len(all_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc3f1d",
   "metadata": {},
   "source": [
    "## 3. Create embeddings & build a vector database (FAISS)\n",
    "\n",
    "### What are embeddings? (We've been using them since session 3)\n",
    "\n",
    "**Embeddings** convert text into numerical vectors (arrays of numbers) that capture semantic meaning:\n",
    "- Similar texts â†’ similar vectors\n",
    "- Enables mathematical similarity comparisons\n",
    "- Typical dimensions: 384, 768, or 1536 numbers per chunk\n",
    "\n",
    "### Vector Database (FAISS)\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) is a library for efficient similarity search:\n",
    "- Stores all chunk embeddings\n",
    "- Quickly finds the most similar chunks to a query\n",
    "- Works entirely offline (no API needed)\n",
    "\n",
    "### Embedding Model Options\n",
    "\n",
    "**Current**: `sentence-transformers/all-mpnet-base-v2`\n",
    "- Dimensions: 768\n",
    "- Quality: High for general-purpose tasks\n",
    "- Speed: Medium\n",
    "\n",
    "**Alternatives:**\n",
    "- `all-MiniLM-L6-v2` â†’ Faster, smaller (384 dim), slightly lower quality (you've already used this one)\n",
    "- `all-mpnet-base-v1` â†’ Similar to v2\n",
    "- OpenAI embeddings â†’ `text-embedding-3-small` (requires API key)\n",
    "\n",
    "**To change**: Just replace the `model_name` parameter in `HuggingFaceEmbeddings()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04cd13e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database saved to: ../vector_databases/vector_db_alice\n"
     ]
    }
   ],
   "source": [
    "VECTOR_DB_DIR = Path(\"../vector_databases\")\n",
    "VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DB_PATH = VECTOR_DB_DIR / \"vector_db_alice\"\n",
    "\n",
    "def create_embedding_vector_db(chunks, db_path: Path):\n",
    "    # 1. Instantiate an embedding model (HuggingFace embeddings)\n",
    "    # 2. Create a FAISS vector store from the chunks\n",
    "    # 3. Save it locally so we can reload it later\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding\n",
    "    )\n",
    "\n",
    "    vectorstore.save_local(str(db_path))\n",
    "    print(f\"Vector database saved to: {db_path}\")\n",
    "\n",
    "create_embedding_vector_db(all_chunks, VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93044d97",
   "metadata": {},
   "source": [
    "### Performance Notes\n",
    "\n",
    "**First run:**\n",
    "- Downloads the embedding model (~420MB for all-mpnet-base-v2)\n",
    "- Creates embeddings for all chunks (may take 1-2 minutes)\n",
    "- Saves the vector database to disk\n",
    "\n",
    "**Subsequent runs:**\n",
    "- Model is cached locally\n",
    "- Can skip this step if vector database already exists\n",
    "- Just load the saved database (next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a98fc2",
   "metadata": {},
   "source": [
    "## 4. Build a retriever from the vector database\n",
    "\n",
    "To use RAG, we need a **retriever** object that:\n",
    "\n",
    "1. Takes a user question  \n",
    "2. Converts it to an embedding (using the same model as the chunks)\n",
    "3. Finds the **k most similar chunks** in the vector store using cosine similarity\n",
    "4. Returns those chunks to be passed to the LLM\n",
    "\n",
    "### The `k` Parameter\n",
    "\n",
    "**`k=4`** means \"retrieve the 4 most similar chunks\"\n",
    "\n",
    "**Trade-offs:**\n",
    "- **Low k (1-3)**: Faster, more focused, but might miss relevant information\n",
    "- **Medium k (4-6)**: Balanced approach (recommended starting point)\n",
    "- **High k (7-15)**: More comprehensive, but may include irrelevant chunks and slow down the LLM\n",
    "\n",
    "**Experiment:** Try different `k` values to see how they affect answer quality and response time.\n",
    "\n",
    "### Search Strategies\n",
    "\n",
    "FAISS supports different search algorithms:\n",
    "- **Similarity search** (default): Returns top-k most similar chunks\n",
    "- **MMR** (Maximum Marginal Relevance): Returns diverse results\n",
    "- **Similarity with score threshold**: Only returns chunks above a certain similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7fcbe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever ready (k=4) from ../vector_databases/vector_db_alice\n"
     ]
    }
   ],
   "source": [
    "def load_retriever(db_path: Path, k: int = 4):\n",
    "    # Reload the FAISS vector store from disk and create a retriever.\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "\n",
    "    vectorstore = FAISS.load_local(\n",
    "        folder_path=str(db_path),\n",
    "        embeddings=embedding,\n",
    "        allow_dangerous_deserialization=True,  # needed in some environments\n",
    "    )\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    print(f\"Retriever ready (k={k}) from {db_path}\")\n",
    "    return retriever\n",
    "\n",
    "alice_retriever = load_retriever(VECTOR_DB_PATH, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0fc6f",
   "metadata": {},
   "source": [
    "## 5. Connect retriever + LLM = RAG chain\n",
    "\n",
    "We now create a **retrieval chain** using **LCEL** (LangChain Expression Language):\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "1. **Input** â†’ User's question\n",
    "2. **Retriever** â†’ Fetches relevant chunks from vector database\n",
    "3. **Format** â†’ Combines chunks into context string\n",
    "4. **Prompt** â†’ Creates LLM prompt with context + question\n",
    "5. **LLM** â†’ Generates answer based on context\n",
    "6. **Output Parser** â†’ Extracts clean string from LLM response\n",
    "\n",
    "### Custom Prompt Design\n",
    "\n",
    "Our prompt instructs the LLM to:\n",
    "- Use only the provided context (retrieved chunks)\n",
    "- **Cite specific passages** from the books\n",
    "- Include brief quotes to support answers\n",
    "- Avoid making up information not in the context\n",
    "\n",
    "### Prompt Customization Options\n",
    "\n",
    "You can modify the system message to change LLM behavior:\n",
    "- Add stricter citation requirements\n",
    "- Request different answer formats (bullet points, summaries, etc.)\n",
    "- Specify answer length constraints\n",
    "- Add domain-specific instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d118fe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain ready.\n"
     ]
    }
   ],
   "source": [
    "def build_rag_chain(retriever):\n",
    "    # Connects the retriever with an LLM using a custom prompt that asks for references.\n",
    "    \n",
    "    # Custom prompt that instructs the LLM to cite sources\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant answering questions about Lewis Carroll's Alice books.\n",
    "Use the following context to answer the question. Always cite specific passages from the books in your answer.\n",
    "When you use information from the context, include a brief quote or reference to show where it came from.\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    # Build RAG chain using LCEL\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "alice_rag_chain = build_rag_chain(alice_retriever)\n",
    "print(\"RAG chain ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeef065",
   "metadata": {},
   "source": [
    "### Alternative Prompt Strategies\n",
    "\n",
    "**Without citations** (original hub prompt):\n",
    "```python\n",
    "prompt = hub.pull_prompt(\"langchain-ai/retrieval-qa-chat\")\n",
    "```\n",
    "\n",
    "**With structured output:**\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Answer in this format:\n",
    "    ANSWER: [your answer]\n",
    "    SOURCES: [relevant quotes]\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "```\n",
    "\n",
    "**With confidence levels:**\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Answer the question and rate your confidence (low/medium/high) \n",
    "    based on how well the context supports your answer.\"\"\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1140543",
   "metadata": {},
   "source": [
    "## 6. Ask questions about the *Alice* books\n",
    "\n",
    "Now we can **chat with the corpus**!\n",
    "\n",
    "### How It Works\n",
    "\n",
    "When you ask a question:\n",
    "1. Question â†’ embedding vector\n",
    "2. Vector database â†’ finds 4 most similar chunks\n",
    "3. Chunks + question â†’ sent to LLM as context\n",
    "4. LLM â†’ generates answer with citations\n",
    "5. Answer â†’ displayed with text wrapping\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- The LLM **does NOT answer from its pretraining alone**\n",
    "- It first retrieves relevant chunks from the *Alice* books\n",
    "- Answers are **grounded in the actual text**\n",
    "- Citations help verify the information\n",
    "\n",
    "### Evaluation Tips\n",
    "\n",
    "When testing your RAG system, consider:\n",
    "- **Relevance**: Does the answer address the question?\n",
    "- **Accuracy**: Is the information correct per the source?\n",
    "- **Citation quality**: Are quotes/references provided?\n",
    "- **Completeness**: Does it cover all relevant aspects?\n",
    "- **No hallucination**: Does it avoid making up information?\n",
    "\n",
    "Try questions that:\n",
    "- Require specific details (names, events)\n",
    "- Need synthesis across multiple passages\n",
    "- Ask about comparisons between the books\n",
    "- Test the system's limits (questions not answerable from the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e939af60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUESTION:\n",
      "How does Alice feel when she falls down the rabbit hole?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ANSWER:\n",
      "\n",
      "When Alice falls down the rabbit hole, she doesn't seem to feel any fear or anxiety about her\n",
      "situation. In fact, she is described as being \"never once considering how in the world she was to\n",
      "get out again\" (CHAPTER I. Down the Rabbit-Hole). This suggests that she is somewhat detached from\n",
      "her predicament and is more focused on her immediate surroundings.  As she falls, Alice experiences\n",
      "a sense of disorientation and confusion, but it doesn't seem to evoke any strong emotions. She is\n",
      "simply swept up in the moment and carried along by the rabbit hole's sudden descent (CHAPTER I. Down\n",
      "the Rabbit-Hole).  It's only when she finds herself in the long, low hall lit by lamps that Alice\n",
      "begins to feel a sense of unease and disorientation. However, even then, her emotions are more those\n",
      "of frustration and disappointment rather than fear or panic.  As she tries to climb up the table leg\n",
      "to retrieve the golden key, Alice's feelings become more intense, and she eventually sits down and\n",
      "cries (CHAPTER I. Down the Rabbit-Hole). This suggests that while she may not have felt fear or\n",
      "anxiety initially, her situation is starting to take its toll on her emotions.  Overall, Alice's\n",
      "initial reaction to falling down the rabbit hole is one of detachment and curiosity, rather than\n",
      "fear or panic.\n",
      "\n",
      "QUESTION:\n",
      "What differences are there between Wonderland and the world behind the looking-glass?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ANSWER:\n",
      "\n",
      "In Lewis Carroll's \"Through the Looking-Glass,\" the differences between Wonderland and the world\n",
      "behind the looking-glass are starkly illustrated through Alice's experiences.  On one hand,\n",
      "Wonderland is a fantastical realm that exists parallel to the real world. As Alice notes, \"she knew\n",
      "she had but to open them again, and all would change to dull reality\" (Chapter 1). This suggests\n",
      "that Wonderland is a separate entity from the everyday world, with its own unique laws and logic.\n",
      "In contrast, the world behind the looking-glass appears to be a reflection of the real world. Alice\n",
      "observes that \"the very first thing she did was to look whether there was a fire in the fireplace\"\n",
      "(Chapter 1), indicating that this world is familiar and mundane. The fact that the fire is \"blazing\n",
      "away as brightly as the one she had left behind\" (Chapter 1) further emphasizes the similarity\n",
      "between these two worlds.  Furthermore, Alice's interactions with the Red Queen and other creatures\n",
      "in Wonderland are often at odds with the behavior of their counterparts in the real world. For\n",
      "example, when Alice tries to get the kitten to imitate the Red Queen, it fails miserably, leading\n",
      "Alice to threaten to send it through the looking-glass (Chapter 1). This suggests that the rules of\n",
      "Wonderland do not necessarily apply to the real world.  However, as Alice navigates both worlds, she\n",
      "begins to realize that they are not entirely separate. She notes that \"the lowing of the cattle in\n",
      "the distance would take the place of the Mock Turtle's heavy sobs\" (Chapter 1), implying that there\n",
      "is a connection between the two worlds. This blurring of boundaries highlights the idea that\n",
      "Wonderland and the real world are intertwined, if only subtly.  Ultimately, the differences between\n",
      "Wonderland and the world behind the looking-glass serve to underscore the fantastical nature of\n",
      "Carroll's narrative. By presenting these two worlds as distinct yet interconnected, Carroll creates\n",
      "a sense of wonder and curiosity in his readers, inviting them to explore the possibilities of both\n",
      "the real and imaginary realms.\n",
      "\n",
      "QUESTION:\n",
      "How is Alice referred to in both books?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ANSWER:\n",
      "\n",
      "In both \"Alice's Adventures in Wonderland\" and \"Through the Looking-Glass\", Alice is referred to as\n",
      "\"Alice\" or \"Little Alice\". In \"Down the Rabbit-Hole\" (CHAPTER I), she is described as a young girl\n",
      "sitting by her sister on the bank, thinking about getting up to pick daisies. This passage\n",
      "highlights her boredom and desire for something exciting to happen.  In \"Tweedledum And Tweedledee\"\n",
      "(CHAPTER IV), Alice is referred to as \"Little Alice\" when she is talking to herself, recalling\n",
      "memories of her little sister's dream. This emphasizes her youthful and innocent nature.  When Alice\n",
      "meets the two fat little men, Tweedledum and Tweedledee, in this chapter, they address her as\n",
      "\"Alice\". This marks a turning point in their interaction, as they begin to engage with her directly.\n",
      "Throughout both books, Alice is often referred to by her nickname or title, emphasizing her small\n",
      "size and childlike wonder. As the story progresses, she grows and evolves, but these early\n",
      "references to her youthfulness remain an important aspect of her character.  As Lewis Carroll writes\n",
      "in \"Down the Rabbit-Hole\", \"Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank\" (CHAPTER I). This passage sets the tone for Alice's journey into Wonderland, where she will\n",
      "encounter a world full of strange and fantastical creatures.\n"
     ]
    }
   ],
   "source": [
    "def ask_alice(question: str, chain=alice_rag_chain):\n",
    "    # Send a question to the RAG chain and print a nicely wrapped answer.\n",
    "    print(f\"\\nQUESTION:\\n{question}\\n\" + \"-\"*80)\n",
    "    answer = chain.invoke(question)\n",
    "    print(\"\\nANSWER:\\n\")\n",
    "    print(textwrap.fill(answer, width=100))\n",
    "\n",
    "# Example questions\n",
    "ask_alice(\"How does Alice feel when she falls down the rabbit hole?\")\n",
    "ask_alice(\"What differences are there between Wonderland and the world behind the looking-glass?\")\n",
    "ask_alice(\"How is Alice referred to in both books?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1419194f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Exercises & Extensions\n",
    "\n",
    "### Beginner Level\n",
    "1. Change `k=4` to `k=2` or `k=8` in the retriever and observe the difference\n",
    "2. Modify `chunk_size` and `chunk_overlap` and rebuild the vector database\n",
    "3. Try different temperatures (0.0, 0.5, 1.0) and compare answers\n",
    "4. Ask your own questions about the Alice books\n",
    "\n",
    "### Intermediate Level\n",
    "5. Switch to a different embedding model (e.g., `all-MiniLM-L6-v2`)\n",
    "6. Modify the prompt to request bullet-point answers\n",
    "7. Add a function to show which chunks were retrieved for each question\n",
    "8. Implement a conversation history (multi-turn dialogue)\n",
    "\n",
    "### Advanced Level\n",
    "9. Add metadata to chunks (book name, chapter) and use it in retrieval\n",
    "10. Implement a hybrid search (keyword + semantic)\n",
    "11. Use a different vector database (Chroma, Pinecone)\n",
    "12. Build a Gradio or Streamlit UI for the RAG system\n",
    "13. Evaluate retrieval quality using metrics (precision@k, recall@k)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [FAISS Documentation](https://faiss.ai/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [Ollama Models](https://ollama.com/library)\n",
    "- [RAG Survey Paper](https://arxiv.org/abs/2312.10997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75442753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
